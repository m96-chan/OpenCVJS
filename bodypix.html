<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>BodyPixSample</title>
    <!-- Load TensorFlow.js -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.2"></script>
    <!-- Load BodyPix -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@2.0"></script>
    <script>
        setupCamera = async (videoElement) => {
            videoElement.srcObject = await navigator.mediaDevices.getUserMedia({
                audio: false,
                video: {
                    width: 1280,
                    height: 720
                }
            });
            return new Promise((resolve) => {
                videoElement.onloadedmetadata = () => {
                    videoElement.play();
                    resolve();
                };
            });
        };
        segmentBody = (input, output, net) => {
            async function renderFrame() {
                const partsSegmentation = await net.segmentPersonParts(input);
                const backgroundBlurAmount = 20;
                const edgeBlurAmount = 3;
                const flipHorizontal = false;
                // bodyPix.drawBokehEffect(
                //      output, input, partsSegmentation, backgroundBlurAmount,
                //      edgeBlurAmount, flipHorizontal);

                // bodyPix.blurBodyPart(
                //     output, input, partsSegmentation, [-1, 0, 1],
                //     backgroundBlurAmount, edgeBlurAmount, flipHorizontal
                // )
                customRender(output, input, partsSegmentation);
                requestAnimationFrame(renderFrame);
            }

            renderFrame();
        }
        start = async () => {
            const videoElem = document.querySelector("#web-camera-video");
            const canvasElem = document.querySelector('#web-camera-render');
            await setupCamera(videoElem);
            const net = await bodyPix.load();
            const partsSegmentation = await net.segmentPersonParts(videoElem)
            console.log(partsSegmentation);

            const nose = partsSegmentation.allPoses[0].keypoints.find(x => x.part === 'nose');
            console.log(nose);
            console.log(virtualFace.width)
            console.log(virtualFace.height);

            segmentBody(videoElem, canvasElem, net);
        };

        const buff = new OffscreenCanvas(1280, 720);
        const virtualBack = new Image();
        virtualBack.src = 'sample.jpg';

        const virtualFace = new Image();
        virtualFace.src = 'overface.png';

        function renderImageDataToCanvas(image, canvas) {
            canvas.width = image.width;
            canvas.height = image.height;
            const ctx = canvas.getContext("2d");
            ctx.putImageData(image, 0, 0);
        }

        customRender = (canvas, input, partsSegmentation) => {
            const ctx = canvas.getContext("2d");
            canvas.width = input.width;
            canvas.height = input.height;
            if (Array.isArray(partsSegmentation) && partsSegmentation.length === 0) {
                ctx.drawImage(input, 0, 0, input.width, input.height);
                return
            }
            const backGroundMask = bodyPix.toMask(partsSegmentation,
                {r: 0, g: 0, b: 0, a: 0},
                {r: 0, g: 0, b: 0, a: 255},
                true, [-1, 0, 1]);



            renderImageDataToCanvas(backGroundMask, buff);
            ctx.drawImage(virtualBack, 0, 0);
            ctx.globalCompositeOperation = 'destination-out'
            ctx.drawImage(buff, 0, 0);
            ctx.globalCompositeOperation = 'destination-over'
            ctx.drawImage(input, 0, 0);
            // バーチャル顔
            if (Array.isArray(partsSegmentation.allPoses) && partsSegmentation.allPoses.length > 0) {
                const nose = partsSegmentation.allPoses[0].keypoints.find(x => x.part === 'nose');
                if (nose) {
                    ctx.globalCompositeOperation = 'source-over'
                    // キャンバスに画像かきこんでゴニョゴニョしたほうが良さそう。
                    ctx.drawImage(virtualFace, nose.position.x - 420, nose.position.y - 370);
                }
            }
            //ctx.drawImage(input, 0, 0, input.width, input.height);
            ctx.restore();
        }
    </script>
</head>
<body onload="start()">
<div>
    <video id="web-camera-video" autoplay muted playsinline width="1280" height="720"></video>
</div>
<div>
    <canvas id="web-camera-render"></canvas>
</div>
</body>
</html>